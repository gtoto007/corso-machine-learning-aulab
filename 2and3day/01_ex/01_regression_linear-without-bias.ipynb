{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterazione 1 - w0: 0.000 - loss:14.956521739130435\n",
      "iterazione 2 - w0: 0.010 - loss:14.613639130434782\n",
      "iterazione 3 - w0: 0.020 - loss:14.275426086956523\n",
      "iterazione 4 - w0: 0.030 - loss:13.941882608695652\n",
      "iterazione 5 - w0: 0.040 - loss:13.613008695652173\n",
      "iterazione 6 - w0: 0.050 - loss:13.288804347826087\n",
      "iterazione 7 - w0: 0.060 - loss:12.969269565217392\n",
      "iterazione 8 - w0: 0.070 - loss:12.654404347826086\n",
      "iterazione 9 - w0: 0.080 - loss:12.344208695652172\n",
      "iterazione 10 - w0: 0.090 - loss:12.038682608695652\n",
      "iterazione 11 - w0: 0.100 - loss:11.737826086956522\n",
      "iterazione 12 - w0: 0.110 - loss:11.441639130434782\n",
      "iterazione 13 - w0: 0.120 - loss:11.150121739130435\n",
      "iterazione 14 - w0: 0.130 - loss:10.86327391304348\n",
      "iterazione 15 - w0: 0.140 - loss:10.58109565217391\n",
      "iterazione 16 - w0: 0.150 - loss:10.30358695652174\n",
      "iterazione 17 - w0: 0.160 - loss:10.030747826086957\n",
      "iterazione 18 - w0: 0.170 - loss:9.762578260869565\n",
      "iterazione 19 - w0: 0.180 - loss:9.499078260869565\n",
      "iterazione 20 - w0: 0.190 - loss:9.240247826086955\n",
      "iterazione 21 - w0: 0.200 - loss:8.986086956521739\n",
      "iterazione 22 - w0: 0.210 - loss:8.736595652173913\n",
      "iterazione 23 - w0: 0.220 - loss:8.491773913043476\n",
      "iterazione 24 - w0: 0.230 - loss:8.251621739130433\n",
      "iterazione 25 - w0: 0.240 - loss:8.01613913043478\n",
      "iterazione 26 - w0: 0.250 - loss:7.7853260869565215\n",
      "iterazione 27 - w0: 0.260 - loss:7.559182608695649\n",
      "iterazione 28 - w0: 0.270 - loss:7.3377086956521715\n",
      "iterazione 29 - w0: 0.280 - loss:7.120904347826086\n",
      "iterazione 30 - w0: 0.290 - loss:6.908769565217389\n",
      "iterazione 31 - w0: 0.300 - loss:6.7013043478260865\n",
      "iterazione 32 - w0: 0.310 - loss:6.498508695652172\n",
      "iterazione 33 - w0: 0.320 - loss:6.300382608695649\n",
      "iterazione 34 - w0: 0.330 - loss:6.10692608695652\n",
      "iterazione 35 - w0: 0.340 - loss:5.918139130434781\n",
      "iterazione 36 - w0: 0.350 - loss:5.734021739130432\n",
      "iterazione 37 - w0: 0.360 - loss:5.554573913043477\n",
      "iterazione 38 - w0: 0.370 - loss:5.37979565217391\n",
      "iterazione 39 - w0: 0.380 - loss:5.209686956521737\n",
      "iterazione 40 - w0: 0.390 - loss:5.044247826086953\n",
      "iterazione 41 - w0: 0.400 - loss:4.883478260869562\n",
      "iterazione 42 - w0: 0.410 - loss:4.727378260869562\n",
      "iterazione 43 - w0: 0.420 - loss:4.575947826086953\n",
      "iterazione 44 - w0: 0.430 - loss:4.429186956521735\n",
      "iterazione 45 - w0: 0.440 - loss:4.28709565217391\n",
      "iterazione 46 - w0: 0.450 - loss:4.1496739130434745\n",
      "iterazione 47 - w0: 0.460 - loss:4.0169217391304315\n",
      "iterazione 48 - w0: 0.470 - loss:3.8888391304347794\n",
      "iterazione 49 - w0: 0.480 - loss:3.7654260869565186\n",
      "iterazione 50 - w0: 0.490 - loss:3.646682608695649\n",
      "iterazione 51 - w0: 0.500 - loss:3.532608695652172\n",
      "iterazione 52 - w0: 0.510 - loss:3.4232043478260845\n",
      "iterazione 53 - w0: 0.520 - loss:3.3184695652173892\n",
      "iterazione 54 - w0: 0.530 - loss:3.218404347826085\n",
      "iterazione 55 - w0: 0.540 - loss:3.123008695652172\n",
      "iterazione 56 - w0: 0.550 - loss:3.03228260869565\n",
      "iterazione 57 - w0: 0.560 - loss:2.94622608695652\n",
      "iterazione 58 - w0: 0.570 - loss:2.8648391304347802\n",
      "iterazione 59 - w0: 0.580 - loss:2.7881217391304323\n",
      "iterazione 60 - w0: 0.590 - loss:2.7160739130434766\n",
      "iterazione 61 - w0: 0.600 - loss:2.648695652173911\n",
      "iterazione 62 - w0: 0.610 - loss:2.585986956521737\n",
      "iterazione 63 - w0: 0.620 - loss:2.5279478260869546\n",
      "iterazione 64 - w0: 0.630 - loss:2.474578260869564\n",
      "iterazione 65 - w0: 0.640 - loss:2.4258782608695637\n",
      "iterazione 66 - w0: 0.650 - loss:2.381847826086955\n",
      "iterazione 67 - w0: 0.660 - loss:2.342486956521738\n",
      "iterazione 68 - w0: 0.670 - loss:2.307795652173912\n",
      "iterazione 69 - w0: 0.680 - loss:2.277773913043477\n",
      "iterazione 70 - w0: 0.690 - loss:2.2524217391304338\n",
      "iterazione 71 - w0: 0.700 - loss:2.231739130434782\n",
      "iterazione 72 - w0: 0.710 - loss:2.2157260869565216\n",
      "iterazione 73 - w0: 0.720 - loss:2.2043826086956515\n",
      "iterazione 74 - w0: 0.730 - loss:2.1977086956521736\n",
      "iterazione 75 - w0: 0.740 - loss:2.1957043478260867\n",
      "\n",
      "RISULTATO DEL TRAINING:\n",
      "w0:0.74 + b0:0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "\n",
    "def prediction(X,w,b): \n",
    "    return X*w+b\n",
    "\n",
    "def loss(X,Y,w,b):\n",
    "    error=prediction(X,w,b) - Y\n",
    "    squared_error = error**2\n",
    "    return np.average(squared_error)\n",
    "\n",
    "def training(X,Y,iteration,learning_rate):\n",
    "    w=0.0\n",
    "    b=0.0\n",
    "    for i in range(iteration):\n",
    "        current_loss = loss(X,Y,w,b)\n",
    "        print(f\"iterazione {i+1} - w0: {w:2.3f} - loss:{current_loss}\")\n",
    "        if(loss(X,Y,w+learning_rate,b)<current_loss):\n",
    "            w+=learning_rate\n",
    "        elif(loss(X,Y,w-learning_rate,b)<current_loss):\n",
    "            w-=learning_rate\n",
    "        else:\n",
    "            return w,b\n",
    "    \n",
    "    raise Exception(f\"Non sono riuscito a convergere con {iteration} iterazioni\")\n",
    "        \n",
    "        \n",
    "    \n",
    "# carico il training set \n",
    "X,Y = np.loadtxt(\"training_set_cornetti.txt\",skiprows=1,unpack=True)\n",
    "\n",
    "# avvio il training\n",
    "w0,b0= training(X,Y,10000,0.01)\n",
    "print(f\"\\nRISULTATO DEL TRAINING:\")\n",
    "print(f\"w0:{w0:2.3} + b0:{b0:2.3}\")\n",
    "\n",
    "# disegno il training set \n",
    "#plt.figure(figsize=(10,10))\n",
    "plt.xlabel(\"#prenotazioni\",fontsize=30)\n",
    "plt.ylabel(\"#cornetti\",fontsize=30)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.plot(X, Y, \"bo\")\n",
    "\n",
    "# disegno la linea che meglio approssima i dati di predizione \n",
    "axis_x=[0,15]\n",
    "axis_y=[b0,prediction(axis_x[1],w0,b0)]\n",
    "plt.plot(axis_x, axis_y, '-r', label=f\"y=x*w0+b0\")\n",
    "plt.text(10,0.5,f\"w0={w0:2.3} b0={b0:2.3}\",fontsize=13)\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
